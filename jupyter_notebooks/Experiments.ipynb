{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T20:20:47.846109100Z",
     "start_time": "2023-12-06T20:20:47.506095900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        Date     avgKWH  cloud_cover  sunshine  global_radiation  max_temp  \\\n0 2011-11-25   9.499781          3.0       5.0              52.0      14.0   \n1 2011-11-26  10.267707          4.0       0.7              24.0      13.9   \n2 2011-11-27  10.850805          3.0       5.9              55.0      13.2   \n3 2011-11-28   9.103382          5.0       0.0              15.0      13.9   \n4 2011-11-29   9.274873          6.0       0.0              15.0      12.8   \n\n   mean_temp  min_temp  precipitation  pressure  ...  weekday cos  month sin  \\\n0       11.0       9.5            0.0  102450.0  ...    -0.900969       -0.5   \n1       10.2       6.3            0.0  102580.0  ...    -0.222521       -0.5   \n2       11.8       9.7            0.0  102130.0  ...     0.623490       -0.5   \n3        6.7       0.2            0.0  102270.0  ...     1.000000       -0.5   \n4        8.6       3.3            0.4  100960.0  ...     0.623490       -0.5   \n\n   month cos  Time of Year sin  Time of Year cos  year    season sin  \\\n0   0.866025         -0.596943          0.802284  2011  1.224647e-16   \n1   0.866025         -0.583054          0.812434  2011  1.224647e-16   \n2   0.866025         -0.568992          0.822343  2011  1.224647e-16   \n3   0.866025         -0.554762          0.832009  2011  1.224647e-16   \n4   0.866025         -0.540368          0.841429  2011  1.224647e-16   \n\n   season cos  (-1)volume weighted price     trend  \n0        -1.0                      44.97  0.000000  \n1        -1.0                      42.66  0.001453  \n2        -1.0                      41.20  0.002907  \n3        -1.0                      42.83  0.004360  \n4        -1.0                      46.88  0.005814  \n\n[5 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>avgKWH</th>\n      <th>cloud_cover</th>\n      <th>sunshine</th>\n      <th>global_radiation</th>\n      <th>max_temp</th>\n      <th>mean_temp</th>\n      <th>min_temp</th>\n      <th>precipitation</th>\n      <th>pressure</th>\n      <th>...</th>\n      <th>weekday cos</th>\n      <th>month sin</th>\n      <th>month cos</th>\n      <th>Time of Year sin</th>\n      <th>Time of Year cos</th>\n      <th>year</th>\n      <th>season sin</th>\n      <th>season cos</th>\n      <th>(-1)volume weighted price</th>\n      <th>trend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2011-11-25</td>\n      <td>9.499781</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>52.0</td>\n      <td>14.0</td>\n      <td>11.0</td>\n      <td>9.5</td>\n      <td>0.0</td>\n      <td>102450.0</td>\n      <td>...</td>\n      <td>-0.900969</td>\n      <td>-0.5</td>\n      <td>0.866025</td>\n      <td>-0.596943</td>\n      <td>0.802284</td>\n      <td>2011</td>\n      <td>1.224647e-16</td>\n      <td>-1.0</td>\n      <td>44.97</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2011-11-26</td>\n      <td>10.267707</td>\n      <td>4.0</td>\n      <td>0.7</td>\n      <td>24.0</td>\n      <td>13.9</td>\n      <td>10.2</td>\n      <td>6.3</td>\n      <td>0.0</td>\n      <td>102580.0</td>\n      <td>...</td>\n      <td>-0.222521</td>\n      <td>-0.5</td>\n      <td>0.866025</td>\n      <td>-0.583054</td>\n      <td>0.812434</td>\n      <td>2011</td>\n      <td>1.224647e-16</td>\n      <td>-1.0</td>\n      <td>42.66</td>\n      <td>0.001453</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-11-27</td>\n      <td>10.850805</td>\n      <td>3.0</td>\n      <td>5.9</td>\n      <td>55.0</td>\n      <td>13.2</td>\n      <td>11.8</td>\n      <td>9.7</td>\n      <td>0.0</td>\n      <td>102130.0</td>\n      <td>...</td>\n      <td>0.623490</td>\n      <td>-0.5</td>\n      <td>0.866025</td>\n      <td>-0.568992</td>\n      <td>0.822343</td>\n      <td>2011</td>\n      <td>1.224647e-16</td>\n      <td>-1.0</td>\n      <td>41.20</td>\n      <td>0.002907</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2011-11-28</td>\n      <td>9.103382</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>13.9</td>\n      <td>6.7</td>\n      <td>0.2</td>\n      <td>0.0</td>\n      <td>102270.0</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>-0.5</td>\n      <td>0.866025</td>\n      <td>-0.554762</td>\n      <td>0.832009</td>\n      <td>2011</td>\n      <td>1.224647e-16</td>\n      <td>-1.0</td>\n      <td>42.83</td>\n      <td>0.004360</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2011-11-29</td>\n      <td>9.274873</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>12.8</td>\n      <td>8.6</td>\n      <td>3.3</td>\n      <td>0.4</td>\n      <td>100960.0</td>\n      <td>...</td>\n      <td>0.623490</td>\n      <td>-0.5</td>\n      <td>0.866025</td>\n      <td>-0.540368</td>\n      <td>0.841429</td>\n      <td>2011</td>\n      <td>1.224647e-16</td>\n      <td>-1.0</td>\n      <td>46.88</td>\n      <td>0.005814</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 25 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "test_train_splits = [{\"train\": 0, \"test\": 0} for _ in range(5)]\n",
    "\n",
    "for file in glob(\"../data/test_train_split/*.feather\"):\n",
    "    file_name = re.split(\"\\\\\\\\|/\", file)[-1]\n",
    "    test_train, split = file_name.split(\".\")[0].split(\"_\")\n",
    "\n",
    "    test_train_splits[int(split)][test_train] = pd.read_feather(file)\n",
    "\n",
    "# FIXME: For the sake of this example, we feature engineer a trend (This should really be done in test_train)\n",
    "MIN_DATE = test_train_splits[0][\"train\"][\"Date\"].min()\n",
    "MAX_DATE = test_train_splits[-1][\"train\"][\"Date\"].max()\n",
    "for i in range(len(test_train_splits)):\n",
    "    test_train_splits[i][\"train\"][\"trend\"] = (test_train_splits[i][\"train\"][\"Date\"] - MIN_DATE).dt.days / (MAX_DATE - MIN_DATE).days\n",
    "    test_train_splits[i][\"test\"][\"trend\"] = (test_train_splits[i][\"test\"][\"Date\"] - MIN_DATE).dt.days / (MAX_DATE - MIN_DATE).days\n",
    "\n",
    "test_train_splits[-1][\"train\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T20:20:49.370953500Z",
     "start_time": "2023-12-06T20:20:48.189930800Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'volume weighted price'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3789\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3790\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3791\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'volume weighted price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 58\u001B[0m\n\u001B[0;32m     53\u001B[0m         \u001B[38;5;66;03m# print(f\"Train {i} Shape: {original_test_train_splits[i]['train_x'].shape}\")\u001B[39;00m\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;66;03m# print(f\"Test  {i} Shape: {original_test_train_splits[i]['test_x'].shape}\")\u001B[39;00m\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_test_train_splits\n\u001B[1;32m---> 58\u001B[0m transformed_test_train_splits \u001B[38;5;241m=\u001B[39m \u001B[43mget_test_train_splits_transformed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_train_splits\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[14], line 52\u001B[0m, in \u001B[0;36mget_test_train_splits_transformed\u001B[1;34m(original_test_train_splits, columns)\u001B[0m\n\u001B[0;32m     50\u001B[0m original_test_train_splits \u001B[38;5;241m=\u001B[39m deepcopy(original_test_train_splits)\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(original_test_train_splits)):\n\u001B[1;32m---> 52\u001B[0m     original_test_train_splits[i] \u001B[38;5;241m=\u001B[39m \u001B[43mdo_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal_test_train_splits\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;66;03m# print(f\"Train {i} Shape: {original_test_train_splits[i]['train_x'].shape}\")\u001B[39;00m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;66;03m# print(f\"Test  {i} Shape: {original_test_train_splits[i]['test_x'].shape}\")\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m original_test_train_splits\n",
      "Cell \u001B[1;32mIn[14], line 18\u001B[0m, in \u001B[0;36mdo_transform\u001B[1;34m(test_train_split, columns)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m scaler \u001B[38;5;129;01min\u001B[39;00m SCALER_VARIABLES:\n\u001B[0;32m     17\u001B[0m     encoder \u001B[38;5;241m=\u001B[39m StandardScaler()\n\u001B[1;32m---> 18\u001B[0m     encoder\u001B[38;5;241m.\u001B[39mfit(\u001B[43mtrain_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43mscaler\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mvalues[:,np\u001B[38;5;241m.\u001B[39mnewaxis])\n\u001B[0;32m     19\u001B[0m     train_df[scaler] \u001B[38;5;241m=\u001B[39m encoder\u001B[38;5;241m.\u001B[39mtransform(train_df[scaler]\u001B[38;5;241m.\u001B[39mvalues[:,np\u001B[38;5;241m.\u001B[39mnewaxis])\n\u001B[0;32m     20\u001B[0m     test_df[scaler] \u001B[38;5;241m=\u001B[39m encoder\u001B[38;5;241m.\u001B[39mtransform(test_df[scaler]\u001B[38;5;241m.\u001B[39mvalues[:,np\u001B[38;5;241m.\u001B[39mnewaxis])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3891\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3892\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3893\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3895\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3793\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3794\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3795\u001B[0m     ):\n\u001B[0;32m   3796\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3797\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3798\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3799\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3800\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3801\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'volume weighted price'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "SCALER_VARIABLES = [\"volume weighted price\", \"cloud_cover\", \"sunshine\", \"global_radiation\", \"max_temp\", \"mean_temp\", \"min_temp\", \"precipitation\", \"pressure\", \"snow_depth\", \"days_elapsed\", \"weekday sin\", \"weekday cos\", \"month sin\", \"month cos\", \"Time of Year sin\", \"Time of Year cos\", \"trend\"]\n",
    "DROP_VARIABLES = [\"year\", \"Date\"]\n",
    "OUTPUT_VARIABLE = \"avgKWH\"\n",
    "\n",
    "def do_transform(test_train_split, columns):\n",
    "    train_df = test_train_split[\"train\"].copy()\n",
    "    train_df = train_df.drop(DROP_VARIABLES, axis=1)\n",
    "    test_df = test_train_split[\"test\"].copy()\n",
    "    test_df = test_df.drop(DROP_VARIABLES, axis=1)\n",
    "\n",
    "    scaler_encoders = []\n",
    "    for scaler in SCALER_VARIABLES:\n",
    "        encoder = StandardScaler()\n",
    "        encoder.fit(train_df[scaler].values[:,np.newaxis])\n",
    "        train_df[scaler] = encoder.transform(train_df[scaler].values[:,np.newaxis])\n",
    "        test_df[scaler] = encoder.transform(test_df[scaler].values[:,np.newaxis])\n",
    "        scaler_encoders.append(encoder)\n",
    "\n",
    "    output_encoder = StandardScaler()\n",
    "    output_encoder.fit(train_df[OUTPUT_VARIABLE].values[:, np.newaxis])\n",
    "    train_df[OUTPUT_VARIABLE] = output_encoder.transform(train_df[OUTPUT_VARIABLE].values[:,np.newaxis])\n",
    "    test_df[OUTPUT_VARIABLE] = output_encoder.transform(test_df[OUTPUT_VARIABLE].values[:,np.newaxis])\n",
    "\n",
    "    train_x_df = train_df.drop(OUTPUT_VARIABLE, axis=1)\n",
    "    # Only keep the columns we want\n",
    "    if columns is not None:\n",
    "        train_x_df = train_x_df[columns]\n",
    "\n",
    "    test_train_split[\"train_x\"] = train_x_df.to_numpy()\n",
    "    test_train_split[\"train_y\"] = train_df[OUTPUT_VARIABLE].to_numpy()\n",
    "    \n",
    "    test_x_df = test_df.drop(OUTPUT_VARIABLE, axis=1)\n",
    "    # Only keep the columns we want\n",
    "    if columns is not None:\n",
    "        test_x_df = test_x_df[columns]\n",
    "\n",
    "    test_train_split[\"test_x\"] = test_x_df.to_numpy()\n",
    "    test_train_split[\"test_y\"] = test_df[OUTPUT_VARIABLE].to_numpy()\n",
    "\n",
    "    test_train_split[\"input_encoders\"] = scaler_encoders\n",
    "    test_train_split[\"output_encoder\"] = output_encoder\n",
    "\n",
    "    return test_train_split\n",
    "\n",
    "def get_test_train_splits_transformed(original_test_train_splits, columns=None):\n",
    "    original_test_train_splits = deepcopy(original_test_train_splits)\n",
    "    for i in range(len(original_test_train_splits)):\n",
    "        original_test_train_splits[i] = do_transform(original_test_train_splits[i], columns=columns)\n",
    "        # print(f\"Train {i} Shape: {original_test_train_splits[i]['train_x'].shape}\")\n",
    "        # print(f\"Test  {i} Shape: {original_test_train_splits[i]['test_x'].shape}\")\n",
    "\n",
    "    return original_test_train_splits\n",
    "\n",
    "transformed_test_train_splits = get_test_train_splits_transformed(test_train_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_cross_validation(model, test_train_splits):\n",
    "    rmses = []\n",
    "    mapes = []\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "\n",
    "    for i in range(len(test_train_splits)):\n",
    "        test_train_split = test_train_splits[i]\n",
    "        model.fit(test_train_split[\"train_x\"], test_train_split[\"train_y\"])\n",
    "        y_pred = model.predict(test_train_split[\"test_x\"])\n",
    "        y_pred = test_train_split[\"output_encoder\"].inverse_transform(y_pred[:,np.newaxis])[:,0]\n",
    "\n",
    "        y_true = test_train_split[\"test\"][OUTPUT_VARIABLE].values\n",
    "        \n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "        rmses.append(rmse)\n",
    "        mapes.append(mape)\n",
    "\n",
    "        y_trues.append(y_true)\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "    return rmses, mapes, y_trues, y_preds\n",
    "\n",
    "\n",
    "rmses, mapes, y_trues, y_preds = evaluate_cross_validation(Ridge(), transformed_test_train_splits)\n",
    "print(f\"Avg RMSE: {sum(rmses) / len(rmses):0.5f}\")\n",
    "print(f\"Avg MAPE: {100 * sum(mapes) / len(mapes):0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_best_models(model, test_train_splits, model_name):\n",
    "    model.fit(test_train_splits[-1][\"train_x\"], test_train_splits[-1][\"train_y\"])\n",
    "\n",
    "    y_pred = model.predict(test_train_splits[-1][\"test_x\"])\n",
    "    y_pred = test_train_splits[-1][\"output_encoder\"].inverse_transform(y_pred[:,np.newaxis])\n",
    "\n",
    "    # Plot last test_train_split\n",
    "    plt.plot(test_train_splits[-1][\"train\"][\"Date\"].values, test_train_splits[-1][\"train\"][\"avgKWH\"].values) # Plot the train\n",
    "    plt.plot(test_train_splits[-1][\"test\"][\"Date\"].values, y_trues[-1], c=\"tab:blue\", label=\"Actual\")\n",
    "    plt.plot(test_train_splits[-1][\"test\"][\"Date\"].values, y_pred, c=\"#FF0000\", label=\"Predicted\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Actual vs Predicted {model_name}\")\n",
    "\n",
    "    # TODO: add residual plots here as needed\n",
    "\n",
    "plot_best_models(Ridge(), transformed_test_train_splits, model_name=\"Ridge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from random import choice\n",
    "\n",
    "# Get test train split with only trend\n",
    "baseline_test_train_splits = get_test_train_splits_transformed(test_train_splits, columns=[\"trend\"])\n",
    "# Train model\n",
    "rmses, mapes, y_trues, y_preds = evaluate_cross_validation(Ridge(), transformed_test_train_splits)\n",
    "# Plot results\n",
    "plot_best_models(Ridge(), baseline_test_train_splits, \"Base Model\")\n",
    "print(f\"RMSE: {sum(rmses) / len(rmses):0.2f} MAPE: {sum(mapes) / len(mapes):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test train split for all the data (Used for the remainder of the notebook)\n",
    "all_data_test_train_splits = get_test_train_splits_transformed(test_train_splits, columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from random import choice\n",
    "\n",
    "RANDOM_SAMPLES = 30\n",
    "tests = {} # (alpha) => (avg mape, avg, rmse)\n",
    "\n",
    "ALPHAS = [10_000, 5_000, 1_000, 500.0, 100.0, 50.0, 10.0, 5.0, 1.0, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "for alpha in ALPHAS:\n",
    "    k = (alpha)\n",
    "    if k in tests:\n",
    "        continue\n",
    "\n",
    "    mlp_model = Ridge(alpha)\n",
    "    rmses, mapes, y_trues, y_preds = evaluate_cross_validation(mlp_model, all_data_test_train_splits)\n",
    "    avg_mape = sum(mapes) / len(mapes)\n",
    "    avg_rmse = sum(rmses) / len(rmses)\n",
    "    tests[k] = (avg_mape, avg_rmse)\n",
    "    \n",
    "    print(f\"(alpha={alpha}) => (MAPE={avg_mape}, RMSE={avg_rmse})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [{'alpha': k, 'mape': v[0], 'rmse': v[1]} for k,v in tests.items()]\n",
    "hp_df = pd.DataFrame(rows)\n",
    "hp_df[hp_df[\"rmse\"] == hp_df[\"rmse\"].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_best_models(Ridge(100), all_data_test_train_splits, \"Ridge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_random_range(min=2, max=64):\n",
    "    min_log = math.log(min, 2)\n",
    "    max_log = math.log(max, 2)\n",
    "\n",
    "    rand_log = min_log + (random.random() * (max_log - min_log))\n",
    "    rand = math.pow(2, rand_log)\n",
    "    return rand\n",
    "\n",
    "RANDOM_SAMPLES = 30\n",
    "tests = {} # (layer_1, layer_2, activation) => (avg mape, avg, rmse)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "for i in range(RANDOM_SAMPLES):\n",
    "    layer_1 = int(scaled_random_range(1, 128))\n",
    "    layer_2 = int(scaled_random_range(1, 128))\n",
    "    activation_func = choice([\"relu\", \"tanh\", \"logistic\"])\n",
    "\n",
    "    k = (layer_1, layer_2, activation_func)\n",
    "    if k in tests:\n",
    "        continue\n",
    "\n",
    "    mlp_model = MLPRegressor(hidden_layer_sizes=(layer_1, layer_2), activation=activation_func, early_stopping=True, max_iter=10_000)\n",
    "    rmses, mapes, y_trues, y_preds = evaluate_cross_validation(mlp_model, all_data_test_train_splits)\n",
    "    avg_mape = sum(mapes) / len(mapes)\n",
    "    avg_rmse = sum(rmses) / len(rmses)\n",
    "    tests[k] = (avg_mape, avg_rmse)\n",
    "    \n",
    "    print(f\"Sample {i} (layer_1={layer_1}, layer_2={layer_2}, activation={activation_func}) => (MAPE={avg_mape}, RMSE={avg_rmse})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [{'layer_1': k[0], 'layer_2': k[1], 'activation': k[2], 'mape': v[0], 'rmse': v[1]} for k,v in tests.items()]\n",
    "hp_df = pd.DataFrame(rows)\n",
    "hp_df[hp_df[\"rmse\"] == hp_df[\"rmse\"].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_best_models(MLPRegressor(hidden_layer_sizes=(15, 32), activation=\"logistic\", early_stopping=True, max_iter=10_000), all_data_test_train_splits, \"MLP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

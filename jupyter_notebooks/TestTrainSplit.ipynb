{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Electricity Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "electricity_cost_df = pd.DataFrame()\n",
    "electricity_cost_data_paths = glob(\"../data/clean/2*.xls\")\n",
    "for electricity_cost_data_path in electricity_cost_data_paths:\n",
    "    electricity_cost_df = pd.concat([pd.read_csv(electricity_cost_data_path), electricity_cost_df])\n",
    "electricity_cost_df[\"Datetime\"] = pd.to_datetime(electricity_cost_df[\"Datetime\"], format=\"%d-%b-%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_cost_df.sort_values(\"Datetime\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Energy Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df = pd.read_csv(\"../data/clean/LondonEnergy.xls\")\n",
    "energy_df[\"Date\"] = pd.to_datetime(energy_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "total_starting_rows = len(energy_df.index)\n",
    "energy_df.sort_values(\"Date\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv(\"../data/clean/LondonWeather.xls\")\n",
    "weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"], format=\"%Y%m%d\")\n",
    "weather_df.sort_values(\"date\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(energy_df, electricity_cost_df, left_on=\"Date\", right_on=\"Datetime\")\n",
    "merged_df = pd.merge(merged_df, weather_df, left_on=\"Date\", right_on=\"date\")\n",
    "print(merged_df.columns)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(merged_df.index) == total_starting_rows) # Assert we didn't lose any data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant date columns\n",
    "merged_df = merged_df.drop([\"Datetime\", \"date\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Train Split\n",
    "For the test train split on time series data, we will use a rolling k fold\n",
    "\n",
    "https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection\n",
    "```\n",
    "Split 1: Test [1] Train [2]\n",
    "Split 2: Test [1, 2] Train [3]\n",
    "Split 3: Test [1, 2, 3] Train [4]\n",
    "Split 4: Test [1, 2, 3, 4] Train [5]\n",
    "Split 5: Test [1, 2, 3, 4, 5] Train [6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "first_date = merged_df[\"Date\"].min()\n",
    "merged_df['DateIndex'] = merged_df[\"Date\"].apply(lambda x: (x- first_date).days).astype(int)\n",
    "merged_df = merged_df.reindex(np.arange(len(merged_df.index)))\n",
    "merged_df = merged_df.sort_values(\"DateIndex\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Data Distrubtion by Date\")\n",
    "sns.histplot(merged_df[\"Date\"], bins=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into 6 even segments.\n",
    "SPLIT_COUNT = 6\n",
    "\n",
    "splits = []\n",
    "date_indexes = merged_df[\"DateIndex\"]\n",
    "data_length = len(date_indexes)\n",
    "for i in range(SPLIT_COUNT):\n",
    "    split = date_indexes[int(i * data_length / SPLIT_COUNT):int((i+1) * data_length / SPLIT_COUNT)]\n",
    "    splits.append((split.min(), split.max()))\n",
    "    if i != 0:\n",
    "        if splits[i][0] == splits[i-1][1]:\n",
    "            splits[i] = (splits[i][0] + 1, splits[i][1])\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"split\"] = -1\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"Split {i} has length: {len(merged_df[(merged_df['DateIndex'] >= split[0]) & (merged_df['DateIndex'] <= split[1])].index )}\")\n",
    "    merged_df.loc[(merged_df[\"DateIndex\"] >= split[0]) & (merged_df[\"DateIndex\"] <= split[1]), \"split\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"6 Folds for Test Train Split\")\n",
    "sns.histplot(data=merged_df, x=\"DateIndex\", hue=\"split\", multiple=\"stack\", bins=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/test_train_split\", exist_ok=True)\n",
    "\n",
    "for i in range(len(splits) - 1):\n",
    "    train_df = merged_df[merged_df[\"split\"] <= i]\n",
    "    train_df = train_df.drop([\"DateIndex\", \"split\"], axis=1)\n",
    "    print(f\"Train {i} has length {len(train_df.index)}\")\n",
    "    train_df.to_csv(f\"../data/test_train_split/train_{i}.csv\", index=False)\n",
    "\n",
    "    test_df = merged_df[merged_df[\"split\"] == i+1]\n",
    "    test_df = test_df.drop([\"DateIndex\", \"split\"], axis=1)\n",
    "    print(f\"Test {i} has length {len(test_df.index)}\")\n",
    "    test_df.to_csv(f\"../data/test_train_split/test_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
